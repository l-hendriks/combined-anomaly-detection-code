{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability.python.bijectors import affine_scalar\n",
    "import math as m\n",
    "import random as r\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "import h5py\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense, LeakyReLU, BatchNormalization, Concatenate, Reshape, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy, categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from scipy.stats import multivariate_normal\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_curve\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from keras.datasets import mnist\n",
    "import importlib\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIG_NAMES = {\n",
    "    '1': {\n",
    "        1: \"stop2b1000_neutralino300\",\n",
    "        2: \"glgl1400_neutralino1100\",\n",
    "        28: \"monojet_Zp2000.0_DM_50.0\",\n",
    "        29: \"glgl1600_neutralino800\",\n",
    "        30: \"monotop_200_A\",\n",
    "        31: \"stlp_st1000\",\n",
    "        32: \"sqsq_sq1800_neut800\",\n",
    "        33: \"sqsq1_sq1400_neut800\",\n",
    "        999: \"Secret set\"\n",
    "    },\n",
    "    '2a': {\n",
    "        25: \"chaneut_cha250_neut150\",\n",
    "        26: \"chaneut_cha400_neut200\",\n",
    "        27: \"pp24mt_50\",\n",
    "        28: \"pp23mt_50\",\n",
    "        29: \"gluino_1000.0_neutralino_1.0\",\n",
    "        30: \"chaneut_cha200_neut50\",\n",
    "        31: \"chaneut_cha300_neut100\",\n",
    "        999: \"Secret set\"\n",
    "    },\n",
    "    '2b': {\n",
    "        1: \"pp24mt_50\",\n",
    "        2: \"chaneut_cha200_neut50\",\n",
    "        3: \"stlp_st1000\",\n",
    "        4: \"chacha_cha600_neut200\",\n",
    "        5: \"pp23mt_50\",\n",
    "        6: \"chaneut_cha250_neut150\",\n",
    "        7: \"chacha_cha400_neut60\",\n",
    "        34: \"gluino_1000.0_neutralino_1.0\",\n",
    "        35: \"chacha_cha300_neut140\",\n",
    "        999: \"Secret set\"\n",
    "    },\n",
    "    '3': {\n",
    "        1: \"glgl1600_neutralino800\",\n",
    "        2: \"monojet_Zp2000.0_DM_50.0\",\n",
    "        3: \"gluino_1000.0_neutralino_1.0\",\n",
    "        4: \"stop2b1000_neutralino300\",\n",
    "        5: \"sqsq1_sq1400_neut800\",\n",
    "        6: \"monotop_200_A\",\n",
    "        7: \"monoV_Zp2000.0_DM_1.0\",\n",
    "        8: \"stlp_st1000\",\n",
    "        34: \"sqsq_sq1800_neut800\",\n",
    "        35: \"glgl1400_neutralino1100\",\n",
    "        999: \"Secret set\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIG_TYPES = {\n",
    "    '1': [1, 2, 28, 29, 30, 31, 32, 33, 999],\n",
    "    '2a': [25, 26, 27, 28, 29, 30, 31, 999],\n",
    "    '2b': [1,2,3,4,5,6,7, 34,35, 999],\n",
    "    '3': [1,2,3,4,5,6,7,8,34,35, 999]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "class types:\n",
    "0 none\n",
    "1 j\n",
    "2 b\n",
    "3 g\n",
    "4 e+\n",
    "5 e-\n",
    "6 m+\n",
    "7 m-\n",
    "\"\"\"\n",
    "\n",
    "CLASS = {\n",
    "    'none': 0.,\n",
    "    'j': 1.,\n",
    "    'b': 2.,\n",
    "    'g': 3.,\n",
    "    'e+': 4.,\n",
    "    'e-': 5.,\n",
    "    'm+': 6.,\n",
    "    'm-': 7.\n",
    "}\n",
    "MAX_JET = 7\n",
    "MAX_LEP = 4\n",
    "\n",
    "CHANNELS = ['3']\n",
    "n_epochs = 100\n",
    "\n",
    "def get_data(channel):\n",
    "    train_h5 = h5py.File('../hackathon-anomaly/training_chan'+str(channel)+'.h5','r')\n",
    "    train = [train_h5['clas'][:,:,0], train_h5['reg'][:]]\n",
    "    \n",
    "    test_h5 = h5py.File('../hackathon-anomaly/testing_chan'+str(channel)+'.h5', 'r')\n",
    "    secret_h5 = h5py.File('../hackathon-anomaly/inference_chan'+str(channel)+'.h5', 'r')\n",
    "    \n",
    "    test_bg_clas = []\n",
    "    test_bg_reg = []\n",
    "    \n",
    "    test_sig_clas = []\n",
    "    test_sig_reg = []\n",
    "    \n",
    "    sig_types = []\n",
    "    \n",
    "    \n",
    "    types = test_h5['type']\n",
    "    clas = test_h5['clas']\n",
    "    reg = test_h5['reg']\n",
    "    \n",
    "    for i in tqdm(range(len(types))):\n",
    "        typ = types[i]\n",
    "        if typ in SIG_TYPES[channel]:\n",
    "            # signal\n",
    "            test_sig_clas.append(clas[i,:,0])\n",
    "            test_sig_reg.append(reg[i])\n",
    "            sig_types.append(typ)\n",
    "        else:\n",
    "            # bg\n",
    "            test_bg_clas.append(clas[i,:,0])\n",
    "            test_bg_reg.append(reg[i])\n",
    "            \n",
    "    # Append secret set\n",
    "    secret_clas = secret_h5['clas']\n",
    "    secret_reg = secret_h5['reg']\n",
    "    for i in tqdm(range(len(secret_reg))):\n",
    "        sig_types.append(999)\n",
    "        test_sig_clas.append(secret_clas[i,:,0])\n",
    "        test_sig_reg.append(secret_reg[i])\n",
    "    \n",
    "    test_bg = [np.array(test_bg_clas), np.array(test_bg_reg)]\n",
    "    test_sig = [np.array(test_sig_clas), np.array(test_sig_reg)]\n",
    "        \n",
    "    return train, test_bg, test_sig, sig_types\n",
    "\n",
    "\n",
    "train, test_bg, test_sig, signal_types = get_data('2a')\n",
    "print(train[0])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_R(coords, center=None):\n",
    "    return np.linalg.norm(coords,axis=1)\n",
    "\n",
    "def append_to_result_radius(result_name, scores):\n",
    "    f = open(\"results-radius.csv\", \"a+\")\n",
    "    for typ in scores:\n",
    "        f.write(result_name + \",\" + str(typ) + \",\" + str(scores[typ]) + \"\\n\")\n",
    "\n",
    "def save_model_results(model_name, model, test_bg, test_sig, sig_types):\n",
    "    # Train results\n",
    "    model.set_latent_mean(train)\n",
    "    model.save_training_scores(model_name, train)\n",
    "    \n",
    "    # Radius\n",
    "    model.evaluate_radius(test_bg, test_sig)\n",
    "    append_to_result_radius(model_name, model.score(sig_types))\n",
    "    \n",
    "    # Save results\n",
    "    model.save_scores(model_name)\n",
    "    \n",
    "def save_model_scores(model_name, model, test_bg, test_sig, sig_types):\n",
    "    # Train results\n",
    "    model.set_latent_mean(train)\n",
    "    model.save_training_scores(model_name, train)\n",
    "    model.evaluate_radius(test_bg, test_sig)\n",
    "    model.score_individual(sig_types)\n",
    "    append_to_result_radius(model_name, model.score(sig_types))\n",
    "    \n",
    "\n",
    "\n",
    "def determine_auc_score(test_bg_scores, test_sig_scores):\n",
    "    test_sig_scores = np.array(test_sig_scores)\n",
    "    y_true = np.concatenate((np.zeros(test_bg_scores.shape[0]), np.ones(test_sig_scores.shape[0])))\n",
    "    y_test = np.concatenate((test_bg_scores, test_sig_scores))\n",
    "    return roc_auc_score(y_true, y_test)\n",
    "\n",
    "class VariationalAutoencoderModel():\n",
    "    def __init__(self, filename, D, dim_z, loss_fn, verbose=False):\n",
    "        self.D = D\n",
    "        self.dim_z = dim_z\n",
    "        self.verbose = verbose\n",
    "        self.model_filename = 'models/' + filename + '.h5'\n",
    "        self.filename = filename\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        model = self.build_lhcdata_model()\n",
    "\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "            \n",
    "        self.model = model\n",
    "        return\n",
    "    \n",
    "    def z_log_var_activation(self, x):\n",
    "        return K.sigmoid(x) * 10\n",
    "\n",
    "    def build_lhcdata_model(self):\n",
    "        D = self.D\n",
    "        in_classification = Input(shape=(D,), name='in_classification')\n",
    "        in_regression = Input(shape=(D*4+2,), name='in_regression')\n",
    "        inputs = Concatenate()([in_classification, in_regression])\n",
    "\n",
    "        x = Dense(512, activation='elu')(inputs)\n",
    "        x = Dense(256, activation='elu')(x)\n",
    "        x = Dense(128, activation='elu')(x)\n",
    "        z_mean = Dense(self.dim_z, name='z_mean', activation='linear')(x)\n",
    "\n",
    "        # instantiate encoder model\n",
    "        self.encoder = Model([in_classification, in_regression], [z_mean], name='encoder')\n",
    "\n",
    "        self.encoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return self.encoder\n",
    "    \n",
    "    def train_model(self, train, batch_size=10000):\n",
    "        earlystopper = EarlyStopping(monitor='loss', patience=50, verbose=0, min_delta=1e-7)\n",
    "        checkpointer = ModelCheckpoint(self.model_filename, monitor='loss', verbose=1, save_best_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.000000001)\n",
    "        lr = 1e-3\n",
    "        epoch_num = 0\n",
    "        best_loss = 1e10\n",
    "        cur_patience = 0\n",
    "        max_patience_lr = 5\n",
    "        max_patience_quit = 10\n",
    "        \n",
    "        # Or have 2 output arms with the same output that have different inputs and train like that?\n",
    "        # Or latent space that is much hgiher than inut space, as this problem is much\n",
    "        # more complex?\n",
    "            \n",
    "        res_train = self.model.fit(\n",
    "            train, \n",
    "            self.loss_fn(train[0].shape[0], self.dim_z),\n",
    "            batch_size=batch_size, \n",
    "            epochs=10000,\n",
    "            shuffle=True,\n",
    "            verbose=1 if self.verbose else 0, \n",
    "            callbacks=[earlystopper, reduce_lr, checkpointer]\n",
    "        )\n",
    "        np.savetxt('losses/' + self.filename, res_train.history['loss'])\n",
    "    \n",
    "    def load_weights(self, weight_filename):\n",
    "        self.model.load_weights(weight_filename)\n",
    "        \n",
    "    def set_latent_mean(self, _):\n",
    "        return\n",
    "    \n",
    "    def evaluate_radius(self, test_bg, test_sig, batch_size=10000):\n",
    "        latent_space_bg = self.encoder.predict(test_bg, batch_size=batch_size, verbose=self.verbose)\n",
    "        latent_space_sig = self.encoder.predict(test_sig, batch_size=batch_size, verbose=self.verbose)\n",
    "        \n",
    "        test_bg_scores = get_R(latent_space_bg - self.loss_fn(latent_space_bg.shape[0], self.dim_z))\n",
    "        test_sig_scores = get_R(latent_space_sig - self.loss_fn(latent_space_sig.shape[0], self.dim_z))\n",
    "        \n",
    "        self.radius_bg = test_bg_scores\n",
    "        self.radius_sig = test_sig_scores\n",
    "        \n",
    "        self.test_bg_scores_r = test_bg_scores / self.max_radius\n",
    "        self.test_sig_scores_r = test_sig_scores / self.max_radius\n",
    "        return test_bg_scores, test_sig_scores\n",
    "    \n",
    "    def save_scores(self, model_name):\n",
    "        np.savetxt('scores/radius-bg-' + model_name, self.test_bg_scores_r)\n",
    "        np.savetxt('scores/radius-sig-' + model_name, self.test_sig_scores_r)\n",
    "        \n",
    "    def save_training_scores(self, model_name, train, batch_size=10000):\n",
    "        latent_space_train = self.encoder.predict(train, batch_size=batch_size, verbose=self.verbose)\n",
    "        train_scores_radius = get_R(latent_space_train - self.loss_fn(latent_space_train.shape[0], self.dim_z))\n",
    "        self.max_radius = np.max(train_scores_radius)\n",
    "        np.savetxt('scores/radius-train-' + model_name, train_scores_radius / self.max_radius)\n",
    "\n",
    "\n",
    "    \n",
    "    def score(self, sig_types):\n",
    "        sig_scores = {}\n",
    "        auc_scores = {}\n",
    "        unique_types = np.unique(sig_types)\n",
    "        for typ in unique_types:\n",
    "            sig_scores[typ] = []\n",
    "        \n",
    "        for i in range(len(self.test_sig_scores_r)):\n",
    "            sig_scores[sig_types[i]].append(self.test_sig_scores_r[i])\n",
    "            \n",
    "#         plt.subplot(1,3,1)\n",
    "#         plt.title(\"background\")\n",
    "#         plt.hist(self.test_bg_scores_r, bins=20)\n",
    "#         plt.subplot(1,3,2)\n",
    "#         plt.title(\"signal gluino_1000.0\")\n",
    "#         plt.hist(sig_scores[30], bins=20)\n",
    "#         plt.subplot(1,3,3)\n",
    "#         plt.title(\"Secret\")\n",
    "#         plt.hist(sig_scores[999], bins=20)\n",
    "#         asasdasd\n",
    "            \n",
    "        for typ in unique_types:\n",
    "            auc_scores[typ] = determine_auc_score(self.test_bg_scores_r, sig_scores[typ])\n",
    "        return auc_scores\n",
    "        \n",
    "    def score_individual(self, sig_types):\n",
    "        sig_scores = {}\n",
    "        auc_scores = {}\n",
    "        unique_types = np.unique(sig_types)\n",
    "        for typ in unique_types:\n",
    "            sig_scores[typ] = []\n",
    "        \n",
    "        for i in range(len(self.test_sig_scores_r)):\n",
    "            sig_scores[sig_types[i]].append(self.test_sig_scores_r[i])\n",
    "        for typ in unique_types:\n",
    "            auc_scores[typ] = determine_auc_score(self.test_bg_scores_r, sig_scores[typ])\n",
    "            np.savetxt('best_scores/radius-bg-' + model_name + '-' + str(typ), self.test_bg_scores_r)\n",
    "            np.savetxt('best_scores/radius-sig-' + model_name + '-' + str(typ), sig_scores[typ])\n",
    "        return auc_scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEc(c):\n",
    "    def MSE(dataset_len, dim_z):\n",
    "        output_data = np.zeros((dataset_len, dim_z), np.int32)\n",
    "        output_data.fill(c)\n",
    "        return output_data\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_z = [5,8,13,21,34,55,89,144,233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_experiments(name, loss_fn):\n",
    "    for c in CHANNELS:\n",
    "        print(c, name)\n",
    "        global train\n",
    "        global test_bg\n",
    "        global test_sig\n",
    "        global sig_types\n",
    "        global model_name\n",
    "        train, test_bg, test_sig, sig_types = get_data(c)\n",
    "        for _z in dim_z:\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                sess = tf.Session()\n",
    "                K.set_session(sess)\n",
    "\n",
    "                model_name = name + '_' + c + '_' + str(_z)\n",
    "                model = VariationalAutoencoderModel(model_name, train[0].shape[1], _z, loss_fn)\n",
    "                model.train_model(train)\n",
    "#                 model.load_weights('models/'+model_name+'.h5')\n",
    "                save_model_scores(model_name, model, test_bg, test_sig, sig_types)\n",
    "CHANNELS = ['1','2a','2b','3']\n",
    "for i in range(1):#range(5):\n",
    "    run_experiments('MSE0-run'+str(i), MSEc(0))\n",
    "    run_experiments('MSE1-run'+str(i), MSEc(1))\n",
    "    run_experiments('MSE2-run'+str(i), MSEc(2))\n",
    "    run_experiments('MSE3-run'+str(i), MSEc(3))\n",
    "    run_experiments('MSE4-run'+str(i), MSEc(4))\n",
    "    run_experiments('MSE10-run'+str(i), MSEc(10))\n",
    "    run_experiments('MSE25-run'+str(i), MSEc(25))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
